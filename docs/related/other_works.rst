Some of the Awesome CL works that use Mammoth
=============================================


**_Get in touch if we missed your awesome work!_**


 - `Gradual Divergence for Seamless Adaptation: A Novel Domain Incremental Learning Method (**ICML 2024**) <https://arxiv.org/abs/2305.04769>`_  
 `Original code <https://github.com/NeurAI-Lab/DARE>`_

..

 - `AGILE  - Mitigating Interference in Incremental Learning through Attention-Guided Rehearsal (**CoLLAs 2024**) <https://arxiv.org/abs/2405.13978>`_  
 `Original code <https://github.com/NeurAI-Lab/AGILE>`_

..

 - `Interactive Continual Learning (ICL) (**CVPR 2024**) <https://arxiv.org/abs/2403.02628>`_  
 `Original code <https://github.com/Biqing-Qi/Interactive-continual-Learning-Fast-and-Slow-Thinking>`_

..

 - `Prediction Error-based Classification for Class-Incremental Learning (**ICLR 2024**) <https://arxiv.org/abs/2305.18806>`_  
 `Original code <https://github.com/michalzajac-ml/pec>`_

..

 - `TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion (**NeurIPS 2023**) <https://arxiv.org/abs/2310.08217>`_  
 `Original code <https://github.com/NeurAI-Lab/TriRE>`_

..

 - `Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation (**NeurIPS 2023**) <https://arxiv.org/abs/2310.08855>`_  
 `Original code <https://github.com/lvyilin/AdaB2N>`_

..

 - `A Unified and General Framework for Continual Learning (**ICLR 2024**) <https://arxiv.org/abs/2403.13249>`_  
 `Original code <https://github.com/joey-wang123/CL-refresh-learning>`_

..

 - `Decoupling Learning and Remembering: a Bilevel Memory Framework with Knowledge Projection for Task-Incremental Learning (**CVPR 2023**) <https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Decoupling_Learning_and_Remembering_A_Bilevel_Memory_Framework_With_Knowledge_CVPR_2023_paper.pdf>`_  
 `Original code <https://github.com/SunWenJu123/BMKP>`_

..

 - `Regularizing Second-Order Influences for Continual Learning (**CVPR 2023**) <https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Regularizing_Second-Order_Influences_for_Continual_Learning_CVPR_2023_paper.pdf>`_  
 `Original code <https://github.com/feifeiobama/InfluenceCL>`_

..

 - `Sparse Coding in a Dual Memory System for Lifelong Learning (**CVPR 2023**) <https://arxiv.org/abs/2301.05058>`_  
 `Original code <https://github.com/NeurAI-Lab/SCoMMER>`_

..

 - `A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm (**CVPR 2023**) <https://arxiv.org/abs/2310.12244>`_  
 `Original code <https://github.com/Wang-ML-Lab/unified-continual-learning>`_

..

 - `A Multi-Head Model for Continual Learning via Out-of-Distribution Replay (**CVPR 2023**) <https://arxiv.org/abs/2208.09734>`_  
 `Original code <https://github.com/k-gyuhak/MORE>`_

..

 - `Preserving Linear Separability in Continual Learning by Backward Feature Projection (**CVPR 2023**) <https://arxiv.org/abs/2303.14595>`_  
 `Original code <https://github.com/rvl-lab-utoronto/BFP>`_

..

 - `Complementary Calibration: Boosting General Continual Learning With Collaborative Distillation and Self-Supervision (**TIP 2023**) <https://ieeexplore.ieee.org/document/10002397>`_  
 `Original code <https://github.com/lijincm/CoCa>`_

..

 - `Continual Learning by Modeling Intra-Class Variation (**TMLR 2023**) <https://arxiv.org/abs/2210.05398>`_  
 `Original code <https://github.com/yulonghui/MOCA>`_

..

 - `ConSlide: Asynchronous Hierarchical Interaction Transformer with Breakup-Reorganize Rehearsal for Continual Whole Slide Image Analysis (**ICCV 2023**) <https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_ConSlide_Asynchronous_Hierarchical_Interaction_Transformer_with_Breakup-Reorganize_Rehearsal_for_Continual_ICCV_2023_paper.pdf>`_  
 `Original code <https://github.com/HKU-MedAI/ConSlide>`_

..

 - `CBA: Improving Online Continual Learning via Continual Bias Adaptor (**ICCV 2023**) <https://arxiv.org/abs/2308.06925>`_  
 `Original code <https://github.com/wqza/CBA-online-CL>`_

..

 - `Neuro-Symbolic Continual Learning: Knowledge, Reasoning Shortcuts and Concept Rehearsal (**ICML 2023**) <https://arxiv.org/abs/2302.01242>`_  
 `Original code <https://github.com/ema-marconato/NeSy-CL>`_

..

 - `Learnability and Algorithm for Continual Learning (**ICML 2023**) <https://arxiv.org/abs/2306.12646>`_  
 `Original code <https://github.com/k-gyuhak/CLOOD>`_

..

 - `Pretrained Language Model in Continual Learning: a Comparative Study (**ICLR 2022**) <https://openreview.net/pdf?id=figzpGMrdD>`_  
 `Original code <https://github.com/wutong8023/PLM4CL>`_

..

 - `Representational continuity for unsupervised continual learning (**ICLR 2022**) <https://openreview.net/pdf?id=9Hrka5PA7LW>`_  
 `Original code <https://github.com/divyam3897/UCL>`_

..

 - `Continual Normalization: Rethinking Batch Normalization for Online Continual Learning (**ICLR 2022**) <https://arxiv.org/abs/2203.16102>`_  
 `Original code <https://github.com/phquang/Continual-Normalization>`_

..

 - `Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System (**ICLR 2022**) <https://arxiv.org/abs/2201.12604>`_  
 `Original code <https://github.com/NeurAI-Lab/CLS-ER>`_

..

 - `New Insights on Reducing Abrupt Representation Change in Online Continual Learning (**ICLR 2022**) <https://openreview.net/pdf?id=N8MaByOzUfb>`_  
 `Original code <https://github.com/pclucas14/AML>`_

..

 - `Looking Back on Learned Experiences for Class/Task Incremental Learning (**ICLR 2022**) <https://openreview.net/pdf?id=RxplU3vmBx>`_  
 `Original code <https://github.com/MozhganPourKeshavarz/Cost-Free-Incremental-Learning>`_

..

 - `Task Agnostic Representation Consolidation: a Self-supervised based Continual Learning Approach (**CoLLAs 2022**) <https://arxiv.org/abs/2207.06267>`_  
 `Original code <https://github.com/NeurAI-Lab/TARC>`_

..

 - `Consistency is the key to further Mitigating Catastrophic Forgetting in Continual Learning (**CoLLAs 2022**) <https://arxiv.org/abs/2207.04998>`_  
 `Original code <https://github.com/NeurAI-Lab/ConsistencyCL>`_

..

 - `Self-supervised models are continual learners (**CVPR 2022**) <https://arxiv.org/abs/2112.04215>`_  
 `Original code <https://github.com/DonkeyShot21/cassle>`_

..

 - `Learning from Students: Online Contrastive Distillation Network for General Continual Learning (**IJCAI 2022**) <https://www.ijcai.org/proceedings/2022/0446>`_  
 `Original code <https://github.com/lijincm/OCD-Net>`_

..